"""
This section is from Connect4GameHandler.py
"""

@numba.njit
def adjust_board_state_numba(
    board_state: npt.NDArray[np.float64],
    player_value: int,
    no_cols: int,
    flip: bool,
) -> npt.NDArray[np.float64]:
    board_state = board_state * player_value  # TODO improve this. adjust board so player is always 1 and opponent -1

    if flip:
        # flip board so most discs are on the left side
        half_no_cols = int(no_cols / 2)
        no_discs_left_side = np.count_nonzero(board_state[:, :half_no_cols])
        no_discs_right_side = np.count_nonzero(board_state[:, -half_no_cols:])
        if no_discs_right_side > no_discs_left_side:
            board_state = np.fliplr(board_state)
            flipped_bool = True
        else:
            flipped_bool = False
    else:
        flipped_bool = False

    return board_state, flipped_bool

def save_data(self: "Connect4GameHandler") -> None:
    pass

def adjust_board_state(
    self: "Connect4GameHandler",
    board_state: npt.NDArray[np.float64],
    player_turn: int,
    flip: bool,
) -> npt.NDArray[np.float64]:
    return adjust_board_state_numba(board_state, self.player_values[player_turn], self.game.no_cols, flip)

def adjust_action(self: "Connect4GameHandler", action: int, flipped_bool: int) -> int:
    if flipped_bool:
        action = (action - (self.game.no_cols - 1)) * (-1)
    return action

"""
This section is from MonteCarloTreeSearch.py
"""

def get_optimal_action_and_next_state_hash(node: dict, max_bool: bool):
    best_action = select_node_action_ucb1(node, 0, None, max_bool=max_bool)
    best_action_idx = node["actions_idx"][best_action]
    q_value = node["q_values"][best_action_idx]
    amaf_q_value = node["amaf_q_values"][best_action_idx]
    next_state_hash = node["next_state_hash"][best_action_idx]
    return best_action, q_value, amaf_q_value, next_state_hash

def get_optimal_tree_actions(game: Connect4Game.Connect4, tree: Tree, player: int, next_player: int):
    start_state_hash = hash(game.get_board().tobytes())
    current_node = tree.get_node(start_state_hash)

    players = [player, next_player]
    player_turn = 0

    nodes = []
    state_hashes = []
    q_values = []
    best_actions = []
    amaf_q_values = []

    while True:
        best_action, q_value, amaf_q_value, next_state_hash = get_optimal_action_and_next_state_hash(
            current_node,
            players[player_turn] == player,
        )
        nodes.append(current_node)
        state_hashes.append(next_state_hash)
        q_values.append(q_value)
        amaf_q_values.append(amaf_q_value)
        best_actions.append(best_action)
        if next_state_hash == 0:
            return best_actions, q_values, amaf_q_values, nodes

        player_turn = (player_turn + 1) % 2
        current_node = tree.get_node(next_state_hash)


"""
This section is from main.py
"""
import time

import numpy as np

import Connect4Game as C4Game
import Connect4GameHandler as C4GameHandler
import Connect4Players as C4Players
import DeepQModel


def create_training_data(Handler, no_games=1, possible_rewards=[1, -1]):
    player_focus = 1  # player0 has player_value=1 and is red

    init_states = []
    actions = []
    rewards = []
    next_states = []
    terminations = []

    player_turn = np.random.choice(2)  # the first player is random
    for _ in range(no_games):
        # play game
        Handler.reset_game()
        Handler.play_game(player_turn)

        # analyze game
        for idx in range(len(Handler.player_turns)):
            player_turn = Handler.player_turns[idx]
            if player_turn == player_focus:
                init_state = Handler.states[idx].flatten()
                action = Handler.actions[idx]
                if idx == len(Handler.states) - 2:  # winning move
                    next_state = np.zeros(init_state.shape)
                    reward = possible_rewards[0]
                    termination = 1
                elif idx == len(Handler.states) - 3:  # losing
                    next_state = np.zeros(init_state.shape)
                    reward = possible_rewards[1]
                    termination = 1
                else:
                    next_state = Handler.states[idx + 2].flatten()
                    reward = 0
                    termination = 0

                init_states.append(init_state)
                actions.append(action)
                rewards.append(reward)
                next_states.append(next_state)
                terminations.append(termination)

                player_turn = (player_turn + 1) % 2  # which player starts first

    training_data = [init_states, actions, rewards, next_states, terminations]
    return [np.array(data) for data in training_data]


def combine_training_data(data0, data1, max_elements=int(1e5)):
    new_data = [np.concatenate((tmpdata0, tmpdata1), axis=0) for tmpdata0, tmpdata1 in zip(data0, data1)]
    len_data = new_data[0].shape[0]
    if len_data > max_elements:
        new_data = [data[-max_elements:] for data in new_data]
    return new_data


def create_batch(training_data, batch_size):
    len_training_data = len(training_data[0])
    batch_samples_idx = np.random.choice(len_training_data, size=batch_size, replace=False)
    batch = [data[batch_samples_idx] for data in training_data]
    return batch


def main0():
    player0 = C4Players.RandomPlayer()
    player1 = C4Players.RandomPlayer()

    game = C4Game.Connect4()

    # Create handler
    Handler = C4GameHandler.Connect4GameHandler(game, player0, player1)

    # play game
    np.random.seed(1002)
    Handler.play_game(0, True)
    game.plot_board_state()

    # play 100 games
    np.random.seed(None)
    Handler.reset_game()
    winners = Handler.play_n_games(1000, False)


def main1():
    player0 = C4Players.RandomPlayer()
    player1 = C4Players.RandomPlayer()

    game = C4Game.Connect4()

    # Create handler
    Handler = C4GameHandler.Connect4GameHandler(game, player0, player1)

    # deep Q model
    QModel = DeepQModel.DeepQModel(
        input_dim=game._no_rows * game._no_cols,
        output_dim=game._no_cols,
        no_layers=6,
        units=15,
    )

    # DP player
    DQPlayer0 = C4Players.DQPlayer(QModel.model, epsilon=0.2)
    DQPlayer1 = C4Players.DQPlayer(QModel.model, epsilon=0)

    # DQ player vs random
    HandlerDvD = C4GameHandler.Connect4GameHandler(game, DQPlayer0, DQPlayer1)
    HandlerDvD.reset_game()

    HandlerDvR = C4GameHandler.Connect4GameHandler(game, DQPlayer1, player0)
    HandlerDvR.reset_game()

    Handlers = [HandlerDvD]

    batch_size = 32

    training_data = create_training_data(Handler, no_games=100)
    winners = []
    for _ in range(1000):
        for __ in range(20):
            for tmp_handler in Handlers:
                new_training_data = create_training_data(tmp_handler, no_games=1)
                print(_, __, tmp_handler.winner, len(tmp_handler.actions))
                winners.append(tmp_handler.winner)
                training_data = combine_training_data(training_data, new_training_data, max_elements=int(5e3))
                # get batch
                init_states, actions, rewards, next_states, terminations = create_batch(training_data, batch_size)
                # do gradient step
                QModel.do_gradient_step(init_states, actions, rewards, next_states, terminations)

        QModel.update_target_model()
        print("avg winner:", np.average(winners[-100:]))

    HandlerDvD.reset_game()
    HandlerDvD.play_n_games(10, True)

    HandlerDvR.reset_game()
    winners = HandlerDvR.play_n_games(100, False)
    print(np.average(winners))


def main2():
    random_player = C4Players.RandomPlayer()
    game = C4Game.Connect4()

    player = 1
    next_player = -1
    confidence_value = 1
    max_count = 1e2
    max_depth = 100

    rave0 = 2**0
    rave1 = 2**0
    cv0 = 2**2
    cv1 = 2**2
    tree0 = True
    tree1 = True

    seed = int(time.time())
    np.random.seed(seed)
    print("Seed", seed)

    print("1 - rave:", rave0, "cv:", cv0, "tree:", tree0)
    print("-1 - rave:", rave1, "cv:", cv1, "tree:", tree1)

    MCTSPlayer0 = C4Players.MCTSPlayer(
        game,
        player=1,
        next_player=-1,
        max_count=max_count,
        max_depth=max_depth,
        confidence_value=cv0,
        rave_param=rave0,
        reuse_tree=tree0,
        randomize_action=False,
    )
    MCTSPlayer1 = C4Players.MCTSPlayer(
        game,
        player=-1,
        next_player=1,
        max_count=max_count,
        max_depth=max_depth,
        confidence_value=cv1,
        rave_param=rave1,
        reuse_tree=tree1,
        randomize_action=False,
    )

    Handler = C4GameHandler.Connect4GameHandler(game, MCTSPlayer0, MCTSPlayer1)

    # Handler.play_game(0, True)
    # game.plot_board_state()

    winners = Handler.play_n_games(10, True, flip=False)
    seed += 1
    np.random.seed(seed)
    print(winners)
    print(np.average(winners))

    # best_action = MCTS.MonteCarloTreeSearch(game, player, next_player, 1e3, confidence_value, random_player)


if __name__ == "__main__":
    main2()
